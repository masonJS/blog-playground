# 7. 동시성 설계의 분해 전략 과 ETL,ELT

> 동시성 설계의 핵심 분해 전략(작업 분해, 데이터 분해)이 어떻게 빅데이터 파이프라인의 근간을 이루는지,
> 그리고 Google Cloud Platform에서 이를 어떻게 구현하는지를 정리한 문서입니다.

---

## 1. 동시성 설계의 두 가지 분해 전략

동시성 프로그래밍에서 문제를 병렬로 처리하기 위해 분해(Decomposition)하는 방법은 크게 두 가지로 나뉩니다.

### 1.1 작업 분해 (Task Decomposition)

작업 분해는 수행해야 할 **"작업의 종류"**를 기준으로 나누는 방식입니다. 각 단위가 서로 다른 종류의 일을 수행합니다.

- 레스토랑에 비유하면, 웨이터(주문 접수), 셰프(요리), 서빙 담당(배달)이 각각 **다른 종류의 작업**을 동시에 수행하는 것과 같습니다.
- 핵심 고려 사항은 작업 간 의존 관계 분석, 각 작업의 처리 시간 균형(부하 불균형 방지), 그리고 작업 간 통신 비용입니다.

#### 대표 패턴: 파이프라인 (Pipeline)

파이프라인은 작업 분해의 가장 대표적인 패턴으로, 작업을 여러 단계(Stage)로 나누고 각 단계가 동시에 서로 다른 데이터를 처리합니다.

```
데이터1 → [Stage A] → [Stage B] → [Stage C] → 결과1
           데이터2 → [Stage A] → [Stage B] → [Stage C] → 결과2
                      데이터3 → [Stage A] → [Stage B] → ...
```

Stage A가 데이터2를 처리하는 동안, Stage B는 데이터1을 처리합니다. 각 스테이지가 동시에 동작하므로 전체 처리량(Throughput)이 크게 향상됩니다.

파이프라인에서 가장 중요한 원칙은 **"가장 느린 스테이지가 전체 처리량을 결정한다"**는 것입니다. 이 병목 스테이지를 어떻게 최적화하느냐가 파이프라인 설계의 핵심 과제입니다.

### 1.2 데이터 분해 (Data Decomposition)

데이터 분해는 처리할 **"데이터"**를 기준으로 나누는 방식입니다. 동일한 작업을 데이터의 서로 다른 부분에 대해 병렬로 수행합니다.

- 1000장의 시험지 채점에 비유하면, 10명의 채점자가 100장씩 나눠서 **동일한 채점 작업**을 수행하는 것과 같습니다.
- 데이터를 나누는 전략(균등 분할, 해시 기반, 범위 기반)과 각 조각 간의 독립성이 핵심 고려 사항입니다.

#### 데이터 분할 전략

| 전략 | 설명 | 예시 |
|------|------|------|
| 균등 분할 | 데이터를 동일한 크기로 나눔 | 배열을 N개의 청크로 분리 |
| 해시 기반 분할 | 특정 키에 해시를 적용하여 분배 | MongoDB 샤딩, 파티셔닝 |
| 범위 기반 분할 | 값의 범위로 나눔 | 날짜 범위별 분할 처리 |

#### 대표 패턴 1: 맵 패턴 (Map Pattern)

맵 패턴은 데이터 분해의 가장 기본적인 패턴으로, **동일한 함수를 데이터의 각 요소에 독립적으로 적용**합니다.

```
입력:  [a, b, c, d, e]
함수:  f

워커1 → f(a)    워커2 → f(b)    워커3 → f(c)    워커4 → f(d)    워커5 → f(e)

결과:  [f(a), f(b), f(c), f(d), f(e)]
```

각 요소의 처리가 다른 요소에 전혀 의존하지 않으므로 동기화 메커니즘(락, 세마포어 등)이 필요 없고, 이론적으로 완벽한 병렬화(Embarrassingly Parallel)가 가능합니다. 워커 수에 비례해서 성능이 선형적으로 향상되는 이상적인 특성을 가집니다.

```typescript
// 맵 패턴 적용: 각 사업자 상태 조회가 서로 독립적
const results = await Promise.all(
  businesses.map(biz => checkSingleBusiness(biz))
);
```

맵 패턴이 성립하려면 각 요소의 처리 결과가 오직 해당 요소의 입력에만 의존해야 하고, 처리 순서를 바꿔도 최종 결과가 동일해야 하며, 공유 상태를 읽지도 쓰지도 않아야 합니다. 누적 합계, 중복 제거, 이전 요소 참조 등은 맵 패턴으로 안전하게 병렬화할 수 없습니다.

#### 대표 패턴 2: 맵리듀스 패턴 (MapReduce Pattern)

맵리듀스는 **Map(분산 처리) → Shuffle(재분배) → Reduce(집계)**의 3단계 구조로, 대규모 분산 환경에서의 데이터 분해 패턴입니다. 데이터를 한 번에 평탄하게 나누고, 각 청크에 동일한 Map 함수를 적용한 뒤, 결과를 키 기준으로 모아서 Reduce합니다.

```
[원본 데이터]
  ↓ 한 번에 평탄하게 분할
┌────────┬────────┬────────┐
│ Chunk1 │ Chunk2 │ Chunk3 │   ← Map: 각 청크에 동일한 함수 적용
│  (키,값) │  (키,값) │  (키,값) │       키-값 쌍 출력
└───┬────┴───┬────┴───┬────┘
    └────────┼────────┘
             ↓                  ← Shuffle: 같은 키끼리 모음
┌────────────────────────┐
│ 키A → [값1, 값2, 값3]   │   ← Reduce: 같은 키의 값들을 집계
│ 키B → [값4, 값5]        │
└────────────────────────┘
```

맵 패턴과 달리, 맵리듀스에는 Shuffle이라는 데이터 재분배 단계가 존재합니다. Map의 결과를 키 기준으로 분류하여 같은 키를 가진 데이터를 한 곳에 모으는 과정으로, 네트워크를 통한 데이터 이동이 발생하므로 맵리듀스에서 가장 큰 성능 병목이 됩니다.

#### 포크/조인 vs 맵리듀스

| 구분 | 포크/조인 (Fork/Join) | 맵리듀스 (MapReduce) |
|------|----------------------|---------------------|
| 분할 방식 | 재귀적 (트리 구조) | 평탄 (한 번에 분할) |
| 실행 환경 | 단일 머신, 멀티코어 | 분산 환경, 수백~수천 대 서버 |
| 데이터 이동 | 같은 메모리 내 (포인터 전달) | 네트워크 통신 (Shuffle) |
| 함수 구조 | 분할과 결합이 하나의 재귀 함수 내 공존 | Map과 Reduce가 완전히 분리된 별개 함수 |
| 적합한 규모 | 메모리에 들어가는 데이터 | 단일 머신에 담을 수 없는 대규모 데이터 |
| 대표 구현 | Java ForkJoinPool | Hadoop MapReduce, Spark |

포크/조인은 재귀적으로 데이터를 반복 분할하며 기저 조건에 도달하면 직접 처리 후 역순으로 결합합니다:

```
              [전체 데이터]
              /    Fork    \
        [절반A]            [절반B]
        /  Fork  \         /  Fork  \
    [1/4]    [1/4]     [1/4]    [1/4]   ← 기저 조건: 직접 처리
        \  Join  /         \  Join  /
        [결과A]            [결과B]
              \    Join    /
              [최종 결과]
```

### 1.3 실무에서의 조합

실제 시스템에서는 작업 분해와 데이터 분해를 결합하여 사용합니다. 파이프라인(작업 분해)의 각 스테이지 내부에서 데이터를 병렬 처리(데이터 분해)하는 구조가 가장 흔합니다.

```
[Stage A]  →  [Stage B]  →  [Stage C]       ← 작업 분해 (파이프라인)
                  │
          ┌──────┼──────┐
          ▼      ▼      ▼
       chunk1  chunk2  chunk3                ← 데이터 분해 (병렬 처리)
```

ETL/ELT 파이프라인이 바로 이 조합의 대표적인 실제 사례입니다.

---

## 2. ETL과 ELT 패턴

### 2.1 ETL (Extract → Transform → Load)

ETL은 다양한 소스에서 데이터를 추출하고, **별도의 중간 서버에서 변환한 뒤**, 정제된 결과만 최종 목적지에 적재하는 패턴입니다. 작업 분해의 파이프라인 패턴을 따르며, 각 단계가 서로 다른 종류의 작업을 수행합니다.

#### 각 단계의 역할

**Extract(추출)**: 다양한 소스(RDBMS, API, CSV, 로그 등)에서 원시 데이터를 가져옵니다. 중간 저장소(스테이징 영역)에 원시 데이터를 내려놓는 행위도 이 단계에 포함됩니다. 스테이징은 소스 시스템 부하 최소화, 변환 실패 시 재시도, 여러 변환 작업의 공유 참조 등을 위해 활용됩니다.

**Transform(변환)**: 추출된 데이터를 비즈니스 요구에 맞게 가공합니다. 데이터 정제, 포맷 변환, 집계, 조인, 필터링 등이 이 단계에서 수행되며, ETL에서 가장 연산 집약적인 단계입니다. 별도의 ETL 서버나 분산 처리 엔진(Spark 등)에서 실행됩니다.

**Load(적재)**: 변환된 결과를 **최종 목적지**에 적재합니다. 여기서 최종 목적지란 데이터 웨어하우스, 데이터 마트 등 분석/리포팅에 사용되는 시스템을 의미합니다. 중간 스테이징 영역(Cloud Storage 등)에 데이터를 업로드하는 행위는 Load가 아닌 Extract(스테이징) 단계에 해당합니다.

```
[소스] → 스테이징(GCS) → 변환(Spark) → 적재(BigQuery)
          ↑                ↑              ↑
      Extract의 일부     Transform        Load
      (중간 저장소)      (별도 서버)     (최종 목적지)
```

#### ETL의 병목

파이프라인 패턴의 원칙 "가장 느린 스테이지가 전체 처리량을 결정한다"가 ETL에도 적용됩니다. Transform 단계가 가장 무거운 경우가 많으며, 이 ETL 서버가 전체 파이프라인의 병목이 됩니다. 데이터가 커지면 이 서버를 스케일업해야 하고, 변환 로직이 복잡해지면 처리 시간이 급격히 늘어납니다.

### 2.2 ELT (Extract → Load → Transform)

ELT는 원시 데이터를 먼저 최종 목적지에 적재한 뒤, **목적지 시스템의 연산 능력**을 활용해 변환을 수행하는 패턴입니다. Load와 Transform의 순서가 뒤바뀐 것이 핵심 차이입니다.

#### ELT가 부상한 배경

클라우드 데이터 웨어하우스(BigQuery, Snowflake, Redshift 등)의 발전이 결정적이었습니다. 이들은 스토리지와 컴퓨팅을 분리(Storage-Compute Separation)하는 아키텍처를 채택하여, 스토리지는 저렴하고 사실상 무제한이며, 컴퓨팅은 필요할 때만 스케일 아웃할 수 있습니다. 이런 환경에서는 중간 서버에서 변환할 이유가 없어지고, 웨어하우스의 MPP(Massively Parallel Processing) 엔진으로 변환하는 것이 더 효율적입니다.

#### 파이프라인 관점의 차이

ETL과 ELT 모두 파이프라인 패턴이지만, **병목 스테이지를 어디에 배치하느냐**가 다릅니다.

```
ETL: 가장 무거운 Transform을 → 별도 중간 서버에 배치 (병목 발생 가능)
ELT: 가장 무거운 Transform을 → 가장 강력한 시스템(웨어하우스)에 배치
```

ELT는 파이프라인의 가장 느린 스테이지(Transform)를 가장 강력한 시스템(클라우드 웨어하우스)에 위임하는 전략입니다.

### 2.3 ETL vs ELT 비교

| 구분 | ETL | ELT |
|------|-----|-----|
| 변환 위치 | 별도의 중간 서버 | 최종 목적지 시스템 내부 |
| 적재 데이터 | 변환된 결과만 | 원시 데이터 그대로 |
| 원시 데이터 접근성 | 스테이징(GCS 등)에 파일로 보존 가능하나, 재활용 시 별도 엔진과 코드 필요 | 분석 시스템 내에 테이블로 존재하여 SQL로 즉시 접근 가능 |
| 스케일링 | ETL 서버를 별도로 스케일업/아웃 | 웨어하우스의 자동 스케일링 활용 |
| 민감 데이터 처리 | 적재 전 중간 서버에서 마스킹/암호화 가능 | 적재 후 웨어하우스 내에서 접근 제어 설정 필요 |
| 적합한 상황 | 실시간 스트리밍, 복잡한 비SQL 변환, ML 전처리 | SQL 표현 가능한 변환, 인프라 관리 최소화, 원시 데이터 즉시 접근 필요 |

#### 원시 데이터 보존에 대한 정확한 이해

ETL에서도 스테이징 영역(GCS 등)에 원시 데이터가 남아있으므로 "보존" 자체는 가능합니다. 그러나 ELT의 진짜 장점은 **"분석 시스템 내에서의 원시 데이터 즉시 접근성"**입니다.

ETL에서 원시 데이터를 재활용하려면 GCS에서 원시 파일을 찾고, 새로운 변환 코드를 작성하고, 연산 클러스터를 띄우고, 결과를 다시 적재해야 합니다. 반면 ELT에서는 원시 데이터가 이미 BigQuery 안에 테이블로 존재하므로, SQL 모델 하나만 추가하면 즉시 새로운 분석을 시작할 수 있습니다.

---

## 3. 분산 프레임워크: Hadoop과 Spark

ETL/ELT의 Transform 단계에서 대규모 데이터를 처리하기 위해 분산 프레임워크가 활용됩니다. 맵리듀스 패턴을 대규모 클러스터에서 실행하는 것이 핵심입니다.

### 3.1 Hadoop

#### 등장 배경

Google이 2003~2004년에 발표한 GFS(Google File System)와 MapReduce 논문에서 영감을 받아 탄생했습니다. "저렴한 서버 수천 대를 묶어서 하나의 거대한 컴퓨터처럼 사용하자"는 발상이 핵심이었고, Doug Cutting이 이를 오픈소스로 구현한 것이 Hadoop입니다.

#### 핵심 구성 요소

**HDFS (Hadoop Distributed File System)**: 파일을 일정 크기(기본 128MB)의 블록으로 쪼개서 여러 서버에 분산 저장하는 분산 파일 시스템입니다. 각 블록을 기본 3개씩 복제하여 서버 장애에 대비합니다. NameNode가 메타데이터를, DataNode가 실제 데이터를 관리합니다.

```
원본 파일: access_log.txt (512MB)

  Block 1 (128MB) → Node A, Node C, Node E  (3곳에 복제)
  Block 2 (128MB) → Node B, Node D, Node F
  Block 3 (128MB) → Node A, Node D, Node G
  Block 4 (128MB) → Node C, Node E, Node B
```

**MapReduce 엔진**: HDFS 위에서 맵리듀스 패턴을 실행하는 연산 프레임워크입니다. 핵심 원칙은 **"데이터가 있는 곳으로 연산을 보낸다(Move Computation to Data)"**입니다. 대량 데이터를 네트워크로 이동시키는 대신, 데이터가 저장된 노드에서 직접 Map 작업을 실행합니다.

#### MapReduce 실행 흐름 (HTTP 상태 코드별 요청 수 카운트)

```
[Input Splits - HDFS 블록 단위]
  access_log_part1 → Mapper 1 (Node A에서 로컬 실행)
  access_log_part2 → Mapper 2 (Node B에서 로컬 실행)
  access_log_part3 → Mapper 3 (Node C에서 로컬 실행)

[Map Phase] - 각 노드에서 로컬 실행, 키-값 쌍 출력
  Mapper 1: "200 /api/users" → ("200", 1)
            "404 /api/old"   → ("404", 1)
            "200 /api/items" → ("200", 1)

  Mapper 2: "500 /api/crash" → ("500", 1)
            "200 /api/users" → ("200", 1)

  Mapper 3: "404 /api/gone"  → ("404", 1)
            "200 /api/items" → ("200", 1)

[Shuffle & Sort] - 네트워크를 통해 같은 키끼리 모음
  "200" → [1, 1, 1, 1]  → Reducer A
  "404" → [1, 1]         → Reducer B
  "500" → [1]            → Reducer B

[Reduce Phase] - 같은 키의 값들을 집계
  Reducer A: "200" → 4
  Reducer B: "404" → 2, "500" → 1
```

#### Hadoop의 한계: 반복적 디스크 I/O

Hadoop MapReduce의 치명적인 문제는 **매 단계마다 디스크에 읽고 쓴다**는 것입니다. 단일 Job이면 괜찮지만, 여러 MapReduce를 체이닝하면 매번 중간 결과를 디스크에 쓰고 다시 읽어야 합니다.

```
MapReduce Job 1 → 디스크에 중간 결과 저장
  ↓ (디스크 읽기)
MapReduce Job 2 → 디스크에 중간 결과 저장
  ↓ (디스크 읽기)
MapReduce Job 3 → 최종 결과
```

이 반복적인 디스크 I/O가 극심한 성능 병목이 되며, 이 문제를 해결하기 위해 Spark가 등장했습니다.

### 3.2 Spark

#### 등장 배경

UC Berkeley AMPLab에서 2009년에 시작된 프로젝트로, Hadoop MapReduce의 디스크 병목을 해결하기 위해 설계되었습니다. 핵심 아이디어는 **중간 결과를 디스크가 아닌 메모리에 유지하자**는 것입니다.

```
Hadoop: Job1 → 디스크 → Job2 → 디스크 → Job3  (매번 디스크 I/O)
Spark:  연산1 → 메모리 → 연산2 → 메모리 → 연산3  (메모리에서 처리)
```

#### RDD (Resilient Distributed Dataset)

Spark의 핵심 추상화로, 여러 노드에 분산된 불변(Immutable) 데이터 컬렉션입니다. 메모리에 캐싱할 수 있다는 것이 Hadoop과의 결정적 차이이며, R(Resilient, 탄력적)은 장애 복구 방식을 나타냅니다.

Spark는 데이터를 복제하는 대신 **Lineage(계보)**를 기록합니다. "이 데이터가 어떤 변환을 거쳐 만들어졌는지"의 과정을 추적하여, 노드 장애 시 원본에서 같은 변환을 재실행해서 복구합니다.

```
원본 데이터 (HDFS)
  → filter(statusCode == "500")     ← 변환 과정을 Lineage로 기록
  → map(line => (url, 1))
  → reduceByKey(_ + _)

Node C 장애 → Lineage를 추적하여 해당 파티션만 재계산
```

#### DAG 기반 실행 모델

Hadoop이 Map → Reduce의 고정된 2단계 구조인 반면, Spark는 **DAG(Directed Acyclic Graph)**로 임의의 연산 파이프라인을 자유롭게 구성합니다.

```
Hadoop: Map → Shuffle → Reduce (고정된 2단계)

Spark:  read → filter → map → groupBy → aggregate → sort → take
        (자유롭게 체이닝, 전체가 하나의 DAG로 최적화)
```

Spark는 이 DAG를 분석해서 불필요한 Shuffle 제거, 여러 변환의 단일 단계 병합 등의 최적화를 수행합니다.

#### Lazy Evaluation (지연 평가)

Spark의 변환(Transformation) 연산은 즉시 실행되지 않고, 액션(Action)이 호출될 때 전체 DAG가 한꺼번에 실행됩니다. 이 덕분에 전체 연산 체인을 보고 최적화할 수 있습니다.

```python
# Transformation: 실행 계획만 기록
rdd = sc.textFile("access_log.txt")
errors = rdd.filter(lambda x: "ERROR" in x)
counts = errors.map(lambda x: (x.split()[1], 1))
result = counts.reduceByKey(lambda a, b: a + b)

# Action: 위의 모든 변환이 한꺼번에 최적화되어 실행
result.collect()
```

### 3.3 Hadoop vs Spark 비교

| 구분 | Hadoop MapReduce | Spark |
|------|-----------------|-------|
| 처리 모델 | Map → Reduce 고정 2단계 | DAG 기반 자유로운 연산 체이닝 |
| 중간 데이터 | 매 단계 디스크 읽기/쓰기 | 메모리 유지, 필요 시 디스크 spill |
| 연산 최적화 | 제한적 (고정된 구조) | DAG 분석 기반 자동 최적화 |
| 장애 복구 | 디스크의 중간 결과에서 재시작 | Lineage 추적 후 재계산 |
| 적합한 워크로드 | 단순 대규모 배치 ETL | 반복 알고리즘(ML), 인터랙티브 분석, 스트리밍 |
| 현재 상태 | MapReduce는 사용 감소, HDFS는 건재 | 사실상 분산 처리 표준 |

현재 생태계에서는 Hadoop MapReduce를 직접 사용하는 경우는 크게 줄었고, HDFS(저장) + Spark(연산)의 조합이 일반적입니다. 클라우드 환경에서는 HDFS 대신 S3, GCS 같은 오브젝트 스토리지를 사용하는 추세입니다.

---

## 4. GCP에서의 ETL/ELT 구현

Google Cloud Platform은 ETL/ELT 파이프라인의 각 단계를 담당하는 서비스를 제공합니다. Google Cloud는 공식적으로 ELT를 데이터 통합의 권장 패턴으로 제시하고 있습니다.

### 4.1 GCP 핵심 서비스 맵

| 파이프라인 단계 | 서비스 | 역할 |
|----------------|--------|------|
| Extract (실시간) | **Cloud Pub/Sub** | 실시간 메시지/이벤트 스트리밍 수집 |
| Extract & Load (CDC) | **Datastream** | 소스 DB 변경 사항을 실시간으로 BigQuery에 동기화 |
| Extract & Load (배치) | **BigQuery Data Transfer Service** | SaaS 소스에서 BigQuery로 배치 전송 |
| 스테이징 | **Cloud Storage (GCS)** | 원시 데이터 중간 저장소 (데이터 레이크) |
| Transform (ETL) | **Dataflow** | Apache Beam 기반 배치/스트리밍 변환 엔진 |
| Transform (ETL, 대규모) | **Dataproc** | 관리형 Hadoop/Spark 클러스터, 복잡한 분산 변환 |
| Transform (ELT) | **Dataform** | BigQuery 내부에서 SQL 기반 변환 파이프라인 구성 |
| 오케스트레이션 | **Cloud Composer** | Apache Airflow 기반 워크플로우 스케줄링/의존성 관리 |
| 최종 저장/분석 | **BigQuery** | 서버리스 데이터 웨어하우스 (MPP 엔진) |
| 시각화 | **Looker Studio** | 인터랙티브 대시보드 및 리포트 |

### 4.2 GCP ETL 아키텍처

변환을 BigQuery 외부에서 수행하는 전통적인 패턴입니다.

```
[소스 시스템]
  ├─ MySQL, PostgreSQL
  ├─ REST API
  └─ CSV/JSON 파일
        │
        ▼ Extract (추출 + 스테이징)
  ┌──────────────────────────────┐
  │  Cloud Pub/Sub (실시간 이벤트) │
  │  Cloud Storage (파일 기반)     │
  └────────────┬─────────────────┘
               │
               ▼ Transform (변환)
  ┌──────────────────────────────┐
  │  Dataflow (Apache Beam)       │  ← 스트리밍/배치 변환
  │  또는                          │
  │  Dataproc (Spark)             │  ← 복잡한 대규모 변환, ML 전처리
  └────────────┬─────────────────┘
               │
               ▼ Load (적재)
  ┌──────────────────────────────┐
  │  BigQuery                     │  ← 최종 목적지: 변환된 결과만 적재
  └────────────┬─────────────────┘
               │
               ▼
  ┌──────────────────────────────┐
  │  Looker Studio                │  ← 시각화/리포팅
  └──────────────────────────────┘

  ※ Cloud Composer가 전체 파이프라인을 오케스트레이션
```

#### ETL에서 Dataflow vs Dataproc 선택 기준

**Dataflow(Apache Beam)**: 스트리밍과 배치를 통합 처리할 수 있는 서버리스 서비스입니다. 인프라 관리가 필요 없고 자동 스케일링이 됩니다. 비교적 단순한 변환이나 실시간 스트리밍 처리에 적합합니다.

**Dataproc(Spark)**: 관리형 Hadoop/Spark 클러스터입니다. Spark의 풍부한 라이브러리 생태계(MLlib, GraphX 등)를 활용할 수 있고, 기존 Spark 코드를 그대로 마이그레이션할 수 있습니다. 복잡한 대규모 조인, ML 파이프라인, 비정형 데이터 처리에 적합합니다.

#### 실시간 ETL 파이프라인 흐름

```
파일 업로드 → GCS
                ↓ (object finalized 이벤트)
           Cloud Function 트리거
                ↓
           Pub/Sub에 메타데이터 발행
                ↓
           Dataflow (스트리밍 모드)
             ├─ 데이터 변환
             └─ BigQuery 스트리밍 삽입 → 랜딩 테이블
                                          ↓
                                    스케줄된 변환 Job
                                          ↓
                                    큐레이션된 테이블
```

### 4.3 GCP ELT 아키텍처

Google Cloud가 권장하는 패턴으로, BigQuery의 MPP 엔진을 활용해 변환을 수행합니다.

```
[소스 시스템]
  ├─ MySQL, PostgreSQL
  ├─ SaaS (Google Ads 등)
  └─ 외부 API
        │
        ▼ Extract & Load (추출 + 즉시 적재)
  ┌──────────────────────────────────┐
  │  Datastream (CDC, 실시간 동기화)    │
  │  BigQuery Data Transfer Service   │  (배치 전체 로드)
  │  Fivetran / Airbyte              │  (서드파티 커넥터)
  │  또는 Dataproc (Spark)            │  (대량 데이터 벌크 이동)
  └────────────┬─────────────────────┘
               │
               ▼
  ┌──────────────────────────────┐
  │  BigQuery (raw tables)        │  ← 원시 데이터 적재 (raw 레이어)
  └────────────┬─────────────────┘
               │
               ▼ Transform (BigQuery 내부에서 변환)
  ┌──────────────────────────────┐
  │  Dataform (SQL 변환 파이프라인) │  ← SQL로 정제, 집계, 모델링
  │  또는 Dataproc (Spark)        │  ← SQL 불가능한 복잡한 변환 보조
  └────────────┬─────────────────┘
               │
               ▼
  ┌──────────────────────────────┐
  │  BigQuery (mart tables)       │  ← 변환된 결과 테이블 (curated 레이어)
  └────────────┬─────────────────┘
               │
               ▼
  ┌──────────────────────────────┐
  │  Looker Studio                │  ← 시각화/리포팅
  └──────────────────────────────┘
```

#### ELT에서 Dataproc의 두 가지 역할

**역할 1 - Extract & Load**: 온프레미스 Hadoop 클러스터, 대량의 Parquet 파일 등 대규모 데이터를 Spark로 읽어서 BigQuery에 벌크 적재하는 데이터 이동 엔진으로 사용됩니다.

**역할 2 - Transform 보조**: ELT의 기본 원칙은 BigQuery + Dataform(SQL)으로 변환하는 것이지만, 자연어 처리(NLP), ML 모델 추론, 이미지 처리 등 SQL로 표현 불가능한 변환이 필요한 경우 Dataproc이 보조적으로 투입됩니다.

### 4.4 ETL vs ELT 패턴 선택 기준 (GCP)

**ELT (BigQuery + Dataform)를 선택하는 경우**: 대부분의 분석/리포팅 워크로드에 해당합니다. SQL로 표현 가능한 변환이 대부분이고, 원시 데이터에 대한 즉시 접근이 필요하며, 인프라 관리를 최소화하고 싶을 때 적합합니다.

**ETL (Dataflow 또는 Dataproc)을 선택하는 경우**: 실시간 스트리밍 처리, 적재 전 민감 데이터 마스킹/암호화, 복잡한 비SQL 변환 로직, ML 모델을 활용한 변환이 필요할 때 적합합니다.

**하이브리드**: 실무에서 가장 흔한 구성입니다. 예를 들어 Dataflow로 실시간 스트리밍 데이터를 수집하면서 기본적인 정제를 수행한 뒤 BigQuery에 적재하고, 이후 Dataform으로 분석용 마트 테이블을 생성하는 구조입니다.

---

## 5. 전체 흐름 요약

```
[동시성 설계 분해 전략]
  ├─ 작업 분해 → 파이프라인 패턴 → ETL/ELT의 단계 구조
  └─ 데이터 분해 → 맵리듀스 패턴 → 각 단계 내부의 병렬 처리
                                     ├─ Hadoop: 디스크 기반 MapReduce
                                     └─ Spark: 메모리 기반 DAG 처리

[GCP 구현]
  ETL: Pub/Sub → GCS(스테이징) → Dataflow/Dataproc(변환) → BigQuery(적재)
  ELT: Datastream/BQ Transfer → BigQuery(적재) → Dataform/Dataproc(변환)
  공통: Cloud Composer(오케스트레이션), Looker Studio(시각화)
```

동시성 프로그래밍의 분해 전략이 단일 프로세스의 스레드 관리에서 시작하여, 분산 시스템의 클러스터 관리, 그리고 클라우드 규모의 데이터 파이프라인까지 동일한 원리로 확장되는 것을 볼 수 있습니다. 규모만 달라질 뿐, **"문제를 나누고 → 병렬로 처리하고 → 결과를 합친다"**는 핵심 아이디어는 동일합니다.
