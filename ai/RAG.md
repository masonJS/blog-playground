## RAG


### RAG (검색 증강 생성)
- 대규모 언어 모델(LLM)이 훈련 데이터 외에 외부 지식 베이스에서 관련 정보를 검색하여 더 정확하고 최신 정보를 반영한 응답을 생성하도록 하는 기술
- 이 기술은 LLM의 환각(할루시네이션)을 줄이고 사용자의 질문에 대해 더 맞춤화되고 검증 가능한 답변을 제공하는 데 유용하며, 챗봇, 검색, 실시간 데이터 요약 등 다양한 분야에 적용

- 작동 방식
1. 외부 데이터 생성
- LLM의 원래 학습 데이터 세트 외부에 있는 새 데이터를 외부 데이터라고 합니다. 
- API, 데이터베이스 또는 문서 리포지토리와 같은 여러 데이터 소스에서 가져올 수 있습니다. 
- 데이터의 형식은 파일, 데이터베이스 레코드 또는 긴 형식의 텍스트와 같이 다양합니다. 
- 임베딩 언어 모델이라고 하는 또 다른 AI 기법은 데이터를 수치로 변환하고 벡터 데이터베이스에 저장합니다. 
- 이 프로세스는 생성형 AI 모델이 이해할 수 있는 지식 라이브러리를 생성

2. 검색 (Retrieval)
- 연관성 검색을 수행하는 단계는 다음과 같습니다. 
- 사용자 쿼리는 벡터 표현으로 변환되고 벡터 데이터베이스와 매칭됩니다. 
- 예를 들어 조직의 인사 관련 질문에 답변할 수 있는 스마트 챗봇을 생각할 수 있습니다. 
- 직원이 “연차휴가는 얼마나 남았나요?“라고 검색하면 시스템은 개별 직원의 과거 휴가 기록과 함께 연차 휴가 정책 문서를 검색합니다. 
- 이러한 특정 문서는 직원이 입력한 내용과 관련이 높기에 반환됩니다. 수학적 벡터 계산 및 표현을 사용하여 연관성이 계산 및 설정됩니다.

3. 증강(Augmentation)
- RAG 모델은 검색된 관련 데이터를 컨텍스트에 추가하여 사용자 입력(또는 프롬프트)을 보강합니다. 
- 이 단계에서는 프롬프트 엔지니어링 기술을 사용하여 LLM과 효과적으로 통신합니다. 
- 확장된 프롬프트를 사용하면 대규모 언어 모델이 사용자 쿼리에 대한 정확한 답변을 생성할 수 있습니다.

4. 생성(Generation)
    - LLM은 제공된 외부 정보와 자체 훈련된 지식을 바탕으로 더 정확하고 구체적인 응답을 생성합니다.

---

### 검색 전략 심화

기본 RAG의 검색 단계는 단순 벡터 유사도 매칭에 의존한다. 하지만 이 방식만으로는 검색 품질에 한계가 있다.

#### Naive RAG의 검색 한계

- **어휘 불일치(Lexical Mismatch)**: 사용자가 "연봉 협상"이라고 질문했는데, 문서에는 "급여 조정"으로 표현되어 있으면 벡터 유사도만으로는 정확한 매칭이 어렵다.
- **노이즈 문서 유입**: 유사도 점수가 높지만 실제로는 관련 없는 문서가 상위에 포함된다.
- **중복 문서 집중**: 상위 K개 결과가 비슷한 내용의 문서로 채워져, 다양한 관점의 정보를 놓친다.
- **Lost in the Middle**: LLM은 컨텍스트의 처음과 끝에 있는 정보에 집중하고, 중간에 위치한 정보는 무시하는 경향이 있다.

이러한 문제를 해결하기 위해 아래 전략들이 사용된다.

---

#### 1. Hybrid Search (하이브리드 검색)

벡터 검색(Semantic Search)과 키워드 검색(Lexical Search)을 결합하는 방식이다.

**왜 필요한가?**

| 검색 방식 | 강점 | 약점 |
|-----------|------|------|
| 벡터 검색 | 의미적으로 유사한 문서를 찾을 수 있다 | 고유명사, 코드, 정확한 수치 매칭에 약하다 |
| 키워드 검색 (BM25) | 정확한 용어 매칭에 강하다 | 동의어, 의역된 표현을 찾지 못한다 |

두 방식을 결합하면 각각의 약점을 보완할 수 있다.

**동작 방식**

```
사용자 쿼리: "NestJS에서 Guards 사용법"

[벡터 검색]                    [키워드 검색 (BM25)]
 - NestJS 인증 미들웨어 가이드    - NestJS Guards 공식 문서
 - Express 미들웨어 패턴          - NestJS Guards 데코레이터 예제
 - NestJS 인터셉터 활용           - Guards vs Middleware 비교

         ↓ 두 결과를 점수 기반으로 병합 ↓

최종 결과 (Reciprocal Rank Fusion 또는 가중 합산)
 1. NestJS Guards 공식 문서
 2. NestJS 인증 미들웨어 가이드
 3. Guards vs Middleware 비교
```

**점수 결합 방식**

- **Reciprocal Rank Fusion (RRF)**: 각 검색 결과의 순위(rank)를 기반으로 점수를 합산한다. 두 검색에서 모두 상위에 등장한 문서가 최종 상위로 올라간다.
  ```
  RRF_score = Σ 1 / (k + rank_i)
  // k는 상수 (보통 60), rank_i는 각 검색에서의 순위
  ```
- **가중 합산 (Weighted Sum)**: 벡터 점수와 키워드 점수에 가중치를 부여하여 합산한다. 도메인 특성에 따라 가중치를 조절할 수 있다.
  ```
  final_score = α × vector_score + (1 - α) × keyword_score
  // α = 0.7이면 의미 검색에 70% 가중
  ```

---

#### 2. Re-ranking (재정렬)

초기 검색(1차 검색)으로 후보 문서를 넓게 가져온 뒤, 별도의 모델이 쿼리와 각 문서의 관련성을 정밀하게 재평가하여 순서를 재정렬하는 방식이다.

**왜 필요한가?**

1차 검색(벡터 검색, BM25 등)은 **속도 우선**으로 설계되어 있다. 쿼리와 문서를 각각 독립적으로 임베딩한 뒤 유사도를 비교하는 Bi-Encoder 구조이기 때문에, 쿼리-문서 간 세밀한 상호작용을 포착하지 못한다.

Re-ranker는 쿼리와 문서를 **함께 입력받아** 관련성 점수를 산출하는 Cross-Encoder 구조를 사용한다. 정확도는 높지만 연산 비용이 크기 때문에 전체 문서가 아닌 1차 검색 결과(Top-N)에만 적용한다.

**동작 방식**

```
[1단계: 초기 검색 - Bi-Encoder (빠름)]
쿼리 → 임베딩 → Top 20 후보 문서 추출

[2단계: Re-ranking - Cross-Encoder (정밀)]
(쿼리, 문서1) → 관련성 점수: 0.95
(쿼리, 문서2) → 관련성 점수: 0.32
(쿼리, 문서3) → 관련성 점수: 0.88
...

→ 점수 기준으로 재정렬 → Top 5를 LLM 컨텍스트에 전달
```

**Bi-Encoder vs Cross-Encoder**

```
Bi-Encoder (1차 검색)          Cross-Encoder (Re-ranking)
┌───────┐  ┌───────┐          ┌─────────────────┐
│ 쿼리   │  │ 문서   │          │ [쿼리] + [문서]  │
└───┬───┘  └───┬───┘          └────────┬────────┘
    ↓          ↓                       ↓
 임베딩      임베딩               Transformer 전체 계층
    ↓          ↓                       ↓
  코사인 유사도 비교              관련성 점수 (0~1)

→ 빠르지만 부정확할 수 있음      → 느리지만 정확함
```

Re-ranking 모델로는 Cohere Rerank, BGE-reranker, ColBERT 등이 사용된다.

---

#### 3. MMR (Maximal Marginal Relevance)

검색 결과에서 **관련성은 높으면서도 서로 중복되지 않는 다양한 문서**를 선택하는 알고리즘이다.

**왜 필요한가?**

단순 유사도 기반 Top-K 검색은 비슷한 내용의 문서가 상위를 독점하는 문제가 있다. 예를 들어 "Redis 캐시 전략"을 검색했을 때 Look Aside 패턴 관련 문서만 5개 반환되면, Write Through나 Write Behind 같은 다른 전략 정보를 놓치게 된다.

MMR은 이미 선택된 문서와의 유사도를 감점 요소로 반영하여, 관련성과 다양성 사이의 균형을 맞춘다.

**수식**

```
MMR = argmax [ λ × Sim(쿼리, 문서i) - (1 - λ) × max(Sim(문서i, 이미 선택된 문서j)) ]
                   ↑                              ↑
              관련성 (Relevance)           중복 페널티 (Diversity)
```

- `λ = 1.0`: 관련성만 고려 (일반 Top-K와 동일)
- `λ = 0.5`: 관련성과 다양성을 균형 있게 고려
- `λ = 0.0`: 다양성만 고려

**동작 예시**

```
쿼리: "Redis 캐시 전략"

일반 Top-K 결과:                MMR 적용 결과 (λ=0.5):
1. Look Aside 캐시 패턴         1. Look Aside 캐시 패턴
2. Look Aside 구현 예제         2. Write Through 전략
3. Look Aside vs Read Through   3. 캐시 무효화 패턴
4. Cache Aside 전략 정리        4. TTL 기반 만료 전략
5. Look Aside 장단점            5. Look Aside vs Read Through

→ 편향된 결과                   → 다양한 관점의 결과
```

---

#### 전략 조합

실무에서는 이 전략들을 파이프라인으로 조합하여 사용한다.

```
사용자 쿼리
    ↓
[Hybrid Search] 벡터 + BM25 → 후보 50건
    ↓
[MMR] 중복 제거 → 후보 20건
    ↓
[Re-ranking] Cross-Encoder 정밀 평가 → 최종 Top 5
    ↓
[LLM] 컨텍스트로 전달 → 응답 생성
```

각 단계를 거칠수록 후보 수는 줄어들고, 품질은 높아진다. Hybrid Search로 recall(재현율)을 확보하고, MMR로 다양성을 보장하며, Re-ranking으로 precision(정밀도)을 높이는 구조다.

